{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Semantic Entropy\n",
    "\n",
    "<div style=\"background-color: rgba(200, 200, 200, 0.1); padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid rgba(127, 127, 127, 0.2); max-width: 97.5%; overflow-wrap: break-word;\">\n",
    "  <p style=\"font-size: 16px; line-height: 1.6\">\n",
    "   Black-box Uncertainty Quantification (UQ) methods treat the LLM as a black box and evaluate consistency of multiple responses generated from the same prompt to estimate response-level confidence. This demo provides an illustration of a state-of-the-art black-box UQ method known as Semantic Entropy.\n",
    "  </p>\n",
    "</div>\n",
    "      \n",
    "## üìä What You'll Do in This Demo\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>1</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section1>Set up LLM and prompts.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Set up LLM instance and load example data prompts.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>2</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section2>Generate LLM Responses and Confidence Scores</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Generate and score LLM responses to the example questions using the <code>SemanticEntropy()</code> class.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 25px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>3</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section3>Evaluate Hallucination Detection Performance</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Visualize model accuracy at different thresholds of the various black-box UQ confidence scores. Compute precision, recall, and F1-score of hallucination detection.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## ‚öñÔ∏è Advantages & Limitations\n",
    "\n",
    "<div style=\"display: flex; gap: 20px\">\n",
    "  <div style=\"flex: 1; background-color: rgba(0, 200, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(0, 200, 0, 0.2)\">\n",
    "    <h3 style=\"color: #2e8b57; margin-top: 0\">Pros</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Universal Compatibility:</strong> Works with any LLM</li>\n",
    "      <li><strong>Intuitive:</strong> Easy to understand and implement</li>\n",
    "      <li><strong>No Internal Access Required:</strong> Doesn't need token probabilities or model internals</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"flex: 1; background-color: rgba(200, 0, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(200, 0, 0, 0.2)\">\n",
    "    <h3 style=\"color: #b22222; margin-top: 0\">Cons</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Higher Cost:</strong> Requires multiple generations per prompt</li>\n",
    "      <li><strong>Slower:</strong> Multiple generations and comparison calculations increase latency</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/uqlm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from uqlm.utils import load_example_dataset, math_postprocessor, plot_model_accuracies, Tuner\n",
    "from uqlm import SemanticEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will illustrate this approach using a set of math questions from the [SVAMP benchmark](https://arxiv.org/abs/2103.07191). To implement with your use case, simply **replace the example prompts with your data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - svamp...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "433ba83e-57aa-47ce-9b1b-22d763691729",
       "rows": [
        [
         "0",
         "There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups How big is each group of bananas?",
         "145"
        ],
        [
         "1",
         "Marco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together their strawberries weighed 30 pounds. How much did Marco's strawberries weigh?",
         "19"
        ],
        [
         "2",
         "Edward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12. How much did each book cost?",
         "3"
        ],
        [
         "3",
         "Frank was reading through his favorite book. The book had 3 chapters, each with the same number of pages. It has a total of 594 pages. It took Frank 607 days to finish the book. How many pages are in each chapter?",
         "198"
        ],
        [
         "4",
         "There were 78 dollars in Olivia's wallet. She spent 15 dollars at a supermarket. How much money does she have left?",
         "63"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are 87 oranges and 290 bananas in Philip...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marco and his dad went strawberry picking. Mar...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Edward spent $ 6 to buy 2 books each book cost...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frank was reading through his favorite book. T...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There were 78 dollars in Olivia's wallet. She ...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer\n",
       "0  There are 87 oranges and 290 bananas in Philip...    145\n",
       "1  Marco and his dad went strawberry picking. Mar...     19\n",
       "2  Edward spent $ 6 to buy 2 books each book cost...      3\n",
       "3  Frank was reading through his favorite book. T...    198\n",
       "4  There were 78 dollars in Olivia's wallet. She ...     63"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (SVAMP)\n",
    "svamp = load_example_dataset(\"svamp\", n=100)\n",
    "svamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "MATH_INSTRUCTION = (\n",
    "    \"When you solve this math problem only return the answer with no additional text.\\n\"\n",
    ")\n",
    "prompts = [MATH_INSTRUCTION + prompt for prompt in svamp.question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `ChatVertexAI` to instantiate our LLM, but any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used. Be sure to **replace with your LLM of choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-google-vertexai\n",
    "# from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# llm = ChatVertexAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "from uqlm.utils.response_generator import LLM\n",
    "\n",
    "llm = LLM(model_name=\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Generate responses and confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SemanticEntropy()` - Generate LLM responses and compute consistency-based confidence scores for each response.\n",
    "\n",
    "#### üìã Class Attributes\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 20%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Parameter</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Type & Default</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 55%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel`. User is responsible for specifying temperature and other relevant parameters to the constructor of the provided `llm` object.</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">device</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or torch.device<br><code>default=\"cpu\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the device that NLI model use for prediction. Only applies to 'semantic_negentropy', 'noncontradiction' scorers. Pass a torch.device to leverage GPU.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_best</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=True</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to swap the original response for the uncertainty-minimized response among all sampled responses based on semantic entropy clusters. Only used if `scorers` includes 'semantic_negentropy' or 'noncontradiction'.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">system_prompt</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=\"You are a helpful assistant.\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument for user to provide custom system prompt for the LLM.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_calls_per_min</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how many API calls to make per minute to avoid rate limit errors. By default, no limit is specified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_n_param</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to use <code>n</code> parameter for <code>BaseChatModel</code>. Not compatible with all <code>BaseChatModel</code> classes. If used, it speeds up the generation process substantially when <code>num_responses</code> is large.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">postprocessor</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">callable<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A user-defined function that takes a string input and returns a string. Used for postprocessing outputs.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">sampling_temperature</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">float<br><code>default=1</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">The 'temperature' parameter for LLM model to generate sampled LLM responses. Must be greater than 0.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">nli_model_name</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"microsoft/deberta-large-mnli\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which NLI model to use. Must be acceptable input to <code>AutoTokenizer.from_pretrained()</code> and <code>AutoModelForSequenceClassification.from_pretrained()</code>.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_length</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=2000</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the maximum allowed string length for LLM responses for NLI computation. Responses longer than this value will be truncated in NLI computations to avoid <code>OutOfMemoryError</code>.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### üîç Parameter Groups\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; margin-bottom: 20px\">\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 100, 200, 0.1); border-radius: 5px; border: 1px solid rgba(0, 100, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üß† LLM-Specific</p>\n",
    "    <ul>\n",
    "      <li><code>llm</code></li>\n",
    "      <li><code>system_prompt</code></li>\n",
    "      <li><code>sampling_temperature</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 200, 0, 0.1); border-radius: 5px; border: 1px solid rgba(0, 200, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üìä Confidence Scores</p>\n",
    "    <ul>\n",
    "      <li><code>nli_model_name</code></li>\n",
    "      <li><code>use_best</code></li>\n",
    "      <li><code>postprocessor</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 150, 0, 0.1); border-radius: 5px; border: 1px solid rgba(200, 150, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üñ•Ô∏è Hardware</p>\n",
    "    <ul>\n",
    "      <li><code>device</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 0, 200, 0.1); border-radius: 5px; border: 1px solid rgba(200, 0, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">‚ö° Performance</p>\n",
    "    <ul>\n",
    "      <li><code>max_calls_per_min</code></li>\n",
    "      <li><code>use_n_param</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "#### üíª Usage Examples\n",
    "\n",
    "```python\n",
    "# Basic usage with default parameters\n",
    "se = SemanticEntropy(llm=llm)\n",
    "\n",
    "# Using GPU acceleration, default scorers\n",
    "se = SemanticEntropy(llm=llm, device=torch.device(\"cuda\"))\n",
    "\n",
    "# High-throughput configuration with rate limiting\n",
    "se = SemanticEntropy(llm=llm, max_calls_per_min=200, use_n_param=True) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the torch device\n",
    "if torch.cuda.is_available():  # NVIDIA GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # macOS\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU\n",
    "print(f\"Using {device.type} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "se = SemanticEntropy(\n",
    "    llm=llm,\n",
    "    max_calls_per_min=250,  # set value to avoid rate limit error\n",
    "    device=device,  # use if GPU available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Class Methods\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">SemanticEntropy.generate_and_score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate LLM responses, sampled LLM (candidate) responses, and compute confidence scores for the provided prompts.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>num_responses</code> - (<strong>int, default=5</strong>) The number of sampled responses used to compute consistency.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Complete end-to-end uncertainty quantification when starting with prompts.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">SemanticEntropy.score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Compute confidence scores on provided LLM responses. Should only be used if responses and sampled responses are already generated.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>responses</code> - (<strong>list of str</strong>) A list of LLM responses for the prompts.</li>\n",
    "        <li><code>sampled_responses</code> - (<strong>list of list of str</strong>) A list of lists of sampled LLM responses for each prompt. These will be used to compute consistency scores by comparing to the corresponding response from <code>responses</code>.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Computing uncertainty scores when responses are already generated elsewhere.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses...\n",
      "Generating candidate responses...\n",
      "Computing confidence scores...\n"
     ]
    }
   ],
   "source": [
    "results = await se.generate_and_score(\n",
    "    prompts=prompts, num_responses=3,\n",
    ")\n",
    "\n",
    "# # alternative approach: directly score if responses already generated\n",
    "# results = se.score(responses=responses, sampled_responses=sampled_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "response",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "entropy_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "confidence_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sampled_responses",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5cf28275-3377-425a-bf86-14e0196518bf",
       "rows": [
        [
         "0",
         "145",
         "0.0",
         "1.0",
         "['145', '145', '145 bananas.']",
         "When you solve this math problem only return the answer with no additional text.\nThere are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups How big is each group of bananas?"
        ],
        [
         "1",
         "19 pounds.",
         "0.0",
         "1.0",
         "['19 pounds.', '19 pounds.', '19 pounds.']",
         "When you solve this math problem only return the answer with no additional text.\nMarco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together their strawberries weighed 30 pounds. How much did Marco's strawberries weigh?"
        ],
        [
         "2",
         "$3",
         "0.0",
         "1.0",
         "['$3', '$3', '$3']",
         "When you solve this math problem only return the answer with no additional text.\nEdward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12. How much did each book cost?"
        ],
        [
         "3",
         "198",
         "0.0",
         "1.0",
         "['198', '198', '198']",
         "When you solve this math problem only return the answer with no additional text.\nFrank was reading through his favorite book. The book had 3 chapters, each with the same number of pages. It has a total of 594 pages. It took Frank 607 days to finish the book. How many pages are in each chapter?"
        ],
        [
         "4",
         "63 dollars.",
         "0.0",
         "1.0",
         "['63', '63 dollars.', '63 dollars']",
         "When you solve this math problem only return the answer with no additional text.\nThere were 78 dollars in Olivia's wallet. She spent 15 dollars at a supermarket. How much money does she have left?"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>entropy_value</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>sampled_responses</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[145, 145, 145 bananas.]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19 pounds.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[19 pounds., 19 pounds., 19 pounds.]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[$3, $3, $3]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[198, 198, 198]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63 dollars.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[63, 63 dollars., 63 dollars]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      response  entropy_value  confidence_score  \\\n",
       "0          145            0.0               1.0   \n",
       "1   19 pounds.            0.0               1.0   \n",
       "2           $3            0.0               1.0   \n",
       "3          198            0.0               1.0   \n",
       "4  63 dollars.            0.0               1.0   \n",
       "\n",
       "                      sampled_responses  \\\n",
       "0              [145, 145, 145 bananas.]   \n",
       "1  [19 pounds., 19 pounds., 19 pounds.]   \n",
       "2                          [$3, $3, $3]   \n",
       "3                       [198, 198, 198]   \n",
       "4         [63, 63 dollars., 63 dollars]   \n",
       "\n",
       "                                              prompt  \n",
       "0  When you solve this math problem only return t...  \n",
       "1  When you solve this math problem only return t...  \n",
       "2  When you solve this math problem only return t...  \n",
       "3  When you solve this math problem only return t...  \n",
       "4  When you solve this math problem only return t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = results.to_df()\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Evaluate Hallucination Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we 'grade' the responses against an answer key. Note the `math_postprocessor` is specific to our use case (math questions). **If you are using your own prompts/questions, update the grading method accordingly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "response",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "entropy_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "confidence_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sampled_responses",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response_correct",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "08c05ae3-2785-4b81-96dd-bfebf84dfa19",
       "rows": [
        [
         "0",
         "145",
         "0.0",
         "1.0",
         "['145', '145', '145 bananas.']",
         "When you solve this math problem only return the answer with no additional text.\nThere are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups How big is each group of bananas?",
         "145",
         "True"
        ],
        [
         "1",
         "19 pounds.",
         "0.0",
         "1.0",
         "['19 pounds.', '19 pounds.', '19 pounds.']",
         "When you solve this math problem only return the answer with no additional text.\nMarco and his dad went strawberry picking. Marco's dad's strawberries weighed 11 pounds. If together their strawberries weighed 30 pounds. How much did Marco's strawberries weigh?",
         "19",
         "True"
        ],
        [
         "2",
         "$3",
         "0.0",
         "1.0",
         "['$3', '$3', '$3']",
         "When you solve this math problem only return the answer with no additional text.\nEdward spent $ 6 to buy 2 books each book costing him the same amount of money. Now he has $ 12. How much did each book cost?",
         "3",
         "True"
        ],
        [
         "3",
         "198",
         "0.0",
         "1.0",
         "['198', '198', '198']",
         "When you solve this math problem only return the answer with no additional text.\nFrank was reading through his favorite book. The book had 3 chapters, each with the same number of pages. It has a total of 594 pages. It took Frank 607 days to finish the book. How many pages are in each chapter?",
         "198",
         "True"
        ],
        [
         "4",
         "63 dollars.",
         "0.0",
         "1.0",
         "['63', '63 dollars.', '63 dollars']",
         "When you solve this math problem only return the answer with no additional text.\nThere were 78 dollars in Olivia's wallet. She spent 15 dollars at a supermarket. How much money does she have left?",
         "63",
         "True"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>entropy_value</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>sampled_responses</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>response_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[145, 145, 145 bananas.]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>145</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19 pounds.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[19 pounds., 19 pounds., 19 pounds.]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[$3, $3, $3]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[198, 198, 198]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>198</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63 dollars.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[63, 63 dollars., 63 dollars]</td>\n",
       "      <td>When you solve this math problem only return t...</td>\n",
       "      <td>63</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      response  entropy_value  confidence_score  \\\n",
       "0          145            0.0               1.0   \n",
       "1   19 pounds.            0.0               1.0   \n",
       "2           $3            0.0               1.0   \n",
       "3          198            0.0               1.0   \n",
       "4  63 dollars.            0.0               1.0   \n",
       "\n",
       "                      sampled_responses  \\\n",
       "0              [145, 145, 145 bananas.]   \n",
       "1  [19 pounds., 19 pounds., 19 pounds.]   \n",
       "2                          [$3, $3, $3]   \n",
       "3                       [198, 198, 198]   \n",
       "4         [63, 63 dollars., 63 dollars]   \n",
       "\n",
       "                                              prompt answer  response_correct  \n",
       "0  When you solve this math problem only return t...    145              True  \n",
       "1  When you solve this math problem only return t...     19              True  \n",
       "2  When you solve this math problem only return t...      3              True  \n",
       "3  When you solve this math problem only return t...    198              True  \n",
       "4  When you solve this math problem only return t...     63              True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate correct answers and grade responses\n",
    "result_df[\"answer\"] = svamp.answer\n",
    "result_df[\"response_correct\"] = [\n",
    "    math_postprocessor(r) == a for r, a in zip(result_df[\"response\"], svamp[\"answer\"])\n",
    "]\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LLM accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Baseline LLM accuracy: {np.mean(result_df[\"response_correct\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Filtered LLM Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explore ‚Äòfiltered accuracy‚Äô as a metric for evaluating the performance of our confidence scores. Filtered accuracy measures the change in LLM performance when responses with confidence scores below a specified threshold are excluded. By adjusting the confidence score threshold, we can observe how the accuracy of the LLM improves as less certain responses are filtered out.\n",
    "\n",
    "We will plot the filtered accuracy across various confidence score thresholds to visualize the relationship between confidence and LLM accuracy. This analysis helps in understanding the trade-off between response coverage (measured by sample size below) and LLM accuracy, providing insights into the reliability of the LLM‚Äôs outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_accuracies(\n",
    "    scores=result_df.confidence_score, correct_indicators=result_df.response_correct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Precision, Recall, F1-Score of Hallucination Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the optimal threshold for binarizing confidence scores, using F1-score as the objective. Using this threshold, we compute precision, recall, and F1-score for black box scorer predictions of whether responses are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic entropy F1-optimal threshold: 0.6\n"
     ]
    }
   ],
   "source": [
    "# instantiate UQLM tuner object for threshold selection\n",
    "t = Tuner()\n",
    "\n",
    "# Define score vector and corresponding correct indicators (i.e. ground truth)\n",
    "y_scores = result_df[\"confidence_score\"]  # confidence score\n",
    "correct_indicators = (\n",
    "    result_df.response_correct\n",
    ") * 1  # Whether responses is actually correct\n",
    "\n",
    "# Solve for threshold that maximizes F1-score\n",
    "best_threshold = t.tune_threshold(\n",
    "    y_scores=y_scores,\n",
    "    correct_indicators=correct_indicators,\n",
    "    thresh_objective=\"fbeta_score\",\n",
    ")\n",
    "y_pred = [\n",
    "    (s > best_threshold) * 1 for s in y_scores\n",
    "]  # predicts whether response is correct based on confidence score\n",
    "print(f\"Semantic entropy F1-optimal threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic entropy precision: 0.8444444444444444\n",
      "Semantic entropy recall: 0.9743589743589743\n",
      "Semantic entropy f1-score: 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "# evaluate precision, recall, and f1-score of semantic entropy predictions of correctness\n",
    "print(\n",
    "    f\"Semantic entropy precision: {precision_score(y_true=correct_indicators, y_pred=y_pred)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Semantic entropy recall: {recall_score(y_true=correct_indicators, y_pred=y_pred)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Semantic entropy f1-score: {f1_score(y_true=correct_indicators, y_pred=y_pred)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2025 CVS Health and/or one of its affiliates. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "uqlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
